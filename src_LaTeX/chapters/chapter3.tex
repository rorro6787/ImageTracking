\documentclass[../main]{subfiles}
\usepackage{lastpage,xr,refcount,etoolbox}
%\externaldocument{../Appendices/Appendix1-CodigosBase}
\begin{document}


\chapter{Image Tracking on Video}

{
\hypersetup{linkcolor=black}
\minitoc
\vspace{5mm}
}
In this chapter, we will use the already trained model to recognize rock-paper-scissors gestures not only in images but also in videos. We will process the video information, identify the gestures, and display the results to demonstrate that the model performs accurately in our tests.

\section{Rock, Paper, Scissors video tracking}
To evaluate our use cases, we have two tasks. The first task, which I am describing now, is to create a script that analyzes a video frame by frame to detect every instance of paper, rock, and scissors. The code in the repository implements all the necessary functionality. We provide pseudocode that illustrates what the function does:
\begin{lstlisting}
def process_video(video_path, buffer_size)
download_model()
open_video()
iniciate_empty_buffer()
    while video_open():
        read_frame()
        if valid_frame:
            model.analize(frame)
            if valid_detection():
                update_buffer(detection)
                if buffer_full():
                    if consistent_data_buffer():
                        process_results(buffer)
                        clean_buffer()
                    else:
                        show_inconsistency_message
            show_frame_with_results()

        if key_to_stop()
            break

    close_video()
    free_resources()
\end{lstlisting}
With the current code implementation, we can already use the trained model to track our desired images in any type of video, as previously mentioned.
\section{Rock, Paper, Scissors versus}
The second feature we implemented extends our existing script. Now that we have a script that can track classes in a video, we’ve created a new one that, given a match in this game, can detect what each competitor has chosen and provide the result of the game. The repository contains a variety of auxiliary functions, but the one responsible for determining the outcome of the match is the following function, which works in conjunction with the one described above.

\begin{lstlisting}
def determine_winner(hand1, hand2):
    hand1 = classes[int(hand1.item())]
    hand2 = classes[int(hand2.item())]

    print(f"Facing {hand1} vs {hand2}\U0001f600")
    time.sleep(1)

    if hand1 == hand2:
        print("It's a draw!")
        print(f"Both hands are {hand1}.")
    elif (hand1 == "rock" and hand2 == "scissors") or \
            (hand1 == "scissors" and hand2 == "paper") or \
            (hand1 == "paper" and hand2 == "rock"):
        print("WE HAVE A WINNER!")
        print(f"{hand1} beats {hand2} :)".upper())
        print(emoji.emojize(":fire:"))
    else:
        print("WE HAVE A WINNER!")
        print(f"{hand2} beats {hand1} :)".upper())
        print(emoji.emojize(":fire:"))
\end{lstlisting}
As you can see, the `hand1` and `hand2` variables that `determine\_winner` receives are tensors or NumPy arrays. They are single-element arrays indicating the index of the class list that the model has detected in the frame being analyzed. That’s why we use `handx.item()`. The rest of the method simply checks whether `hand1` is winning against `hand2` or vice versa. With this and all the auxiliary functions available in the repository, you can upload your own video and use the code to see how the model correctly predicts and determines the winner in your rock, paper, scissors match.





\end{document}



% \stackrel{2}{1} B

% {\textcolor{blue}{\ref{fig:concepto}}}